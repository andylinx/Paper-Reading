# Survey

1. (2023.12) Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis [[Paper]](https://arxiv.org/abs/2312.08782)
1. (2024.4) What Foundation Models can Bring for Robot Learning in Manipulation : A Survey [[Paper]](https://arxiv.org/abs/2404.18201)



# Simulation Platform

1. (2020.9 & 2022.11) **Robosuite**: A Modular Simulation Framework and Benchmark for Robot Learning [[Paper]](https://arxiv.org/abs/2009.12293) [[Project]](https://robosuite.ai/)
2. (2023.3) **FluidLab**: A Differentiable Environment for Benchmarking Complex Fluid Manipulation [[Paper]](https://arxiv.org/abs/2303.02346) [[Project]](https://github.com/zhouxian/FluidLab)
3. (2024.6) **RoboCasa**: Large-Scale Simulation of Everyday Tasks for Generalist Robots  [[Paper]](https://arxiv.org/abs/2406.02523) [[Project]](https://robo-pack.github.io)
4. 



# Large Models

1. (2023.10 & 2024.6) **RT-X Model**: Robotic Learning Datasets and RT-X Models [[Paper]](https://arxiv.org/abs/2310.08864)
2. 



## Physics + Robotic

1. (2019.6) **DensePhysNet**: Learning Dense Physical Object Representations via Multi-step Dynamic Interactions [[Paper]](https://arxiv.org/abs/1906.03853) [[Project]](https://www.zhenjiaxu.com/DensePhysNet/)

2. (2023.4) **3D-IntPhys**: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes [[Paper]](https://arxiv.org/abs/2312.05359)

3. (2023.9 & 2024.3) **PHYSOBJECTS**: Physically Grounded Vision-Language Models for Robotic Manipulation [[Paper]](https://arxiv.org/abs/2309.02561) [[Project]](https://iliad.stanford.edu/pg-vlm/)

4. (2023.12 & 2024.2) Any-point Trajectory Modeling for Policy Learning [[Paper]](https://arxiv.org/abs/2401.00025) [[Project]](https://xingyu-lin.github.io/atm/)

5. (2024.7) **RoboPack**: Learning Tactile-Informed Dynamics Models for Dense Packing [[Paper]](https://arxiv.org/pdf/2407.01418) [[Project]](https://robo-pack.github.io/)

6. (2024.7) **TAPVid-3D**: A Benchmark for Tracking Any Point in 3D [[Paper]](https://arxiv.org/abs/2407.05921) [[Project]](https://tapvid3d.github.io/)

   



# MLLM + Robotic Manipulation



1. (2023.12) **ManipLLM**: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation [[Paper]](https://arxiv.org/abs/2312.16217) [[Project]](https://sites.google.com/view/manipllm)
2. (2024.3) **ManipVQA**: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models [[Paper]](https://arxiv.org/abs/2405.17418) 
3. (2024.5) **Self-Corrected** Multimodal Large Language Model for End-to-End Robot Manipulation [[Paper]](https://arxiv.org/abs/2405.17418) 
4. (2024.5) **SAM-E**: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation [[Paper]](https://arxiv.org/abs/2405.19586) [[Project]](https://sam-embodied.github.io/)
5. (2024.6) **RoboMamba**: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation [[Paper]](https://arxiv.org/abs/2406.04339) [[Project]](https://sites.google.com/view/robomamba-web)
6. (2024.6) **LLaRA**: Supercharging Robot Learning Data for Vision-Language Policy [[Paper]](https://arxiv.org/pdf/2406.20095) [[Project]](https://github.com/LostXine/LLaRA)
7. (2024.6) **ManiWAV**: Learning Robot Manipulation from In-the-Wild Audio-Visual Data [[Paper]](https://arxiv.org/abs/2406.19464) [[Project]](https://mani-wav.github.io/)





# Reinforcement Learning

1. (2024.6) MEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention [[Paper]](https://arxiv.org/abs/2406.16258)





# Energy Based Learning

1. (2021.9) Implicit Behavioral Cloning [[Paper]](https://arxiv.org/pdf/2109.00137)



# Transformer Based Learning

1. (2022.9 & 2022.11) PERCEIVER-ACTOR: A Multi-Task Transformer for Robotic Manipulation [[Paper]](https://arxiv.org/pdf/2209.05451)

2. (2024.6) RVT-2: Learning Precise Manipulation from Few Demonstrations [[Paper]](https://arxiv.org/abs/2406.08545v1) [[Project]](https://robotic-view-transformer-2.github.io/)





# Diffusion Based Learning

1. (2023.1 & 2023.3) Imitating Human Behaviour with Diffusion Models [[Paper\]](https://arxiv.org/abs/2301.10677) [[Project\]](https://github.com/microsoft/Imitating-Human-Behaviour-w-Diffusion)
2. (2023.12) ChainedDiffuser: Unifying Trajectory Diffusion and Keypose Prediction for Robotic Manipulation [[Paper]](https://openreview.net/forum?id=W0zgY2mBTA8) [[Project]](https://chained-diffuser.github.io/)
3. (2024.2) 3D Diffuser Actor: Policy Diffusion with 3D Scene Representations [[Paper]](https://arxiv.org/abs/2402.10885) [[Project]](https://github.com/nickgkan/3d_diffuser_actor?tab=readme-ov-file)
4. (2024.3) Diffusion Policy: Visuomotor Policy Learning via Action Diffusion [[Paper]](https://arxiv.org/pdf/2303.04137) [[Project]](https://diffusion-policy.cs.columbia.edu/)

5. (2024.7) Potential Based Diffusion Motion Planning [[Paper]](https://arxiv.org/pdf/2407.06169) [[Project]](https://energy-based-model.github.io/potential-motion-plan/)
6. 
