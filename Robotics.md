# Survey

1. (2023.12) Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis [[Paper]](https://arxiv.org/abs/2312.08782)
1. (2024.4) What Foundation Models can Bring for Robot Learning in Manipulation : A Survey [[Paper]](https://arxiv.org/abs/2404.18201)



# Simulation Platform

1. (2020.9 & 2022.11) **Robosuite**: A Modular Simulation Framework and Benchmark for Robot Learning [[Paper]](https://arxiv.org/abs/2009.12293) [[Project]](https://robosuite.ai/)
2. (2024.6) **RoboCasa**: Large-Scale Simulation of Everyday Tasks for Generalist Robots  [[Paper]](https://arxiv.org/abs/2406.02523) [[Project]](https://robo-pack.github.io)
3. 



# Large Models

1. (2023.10 & 2024.6) **RT-X Model**: Robotic Learning Datasets and RT-X Models [[Paper]](https://arxiv.org/abs/2310.08864)
2. 



## Physics + Robotic

1. (2019.6) **DensePhysNet**: Learning Dense Physical Object Representations via Multi-step Dynamic Interactions [[Paper]](https://arxiv.org/abs/1906.03853) [[Project]](https://www.zhenjiaxu.com/DensePhysNet/)
2. (2023.9 & 2024.3) **PHYSOBJECTS**: Physically Grounded Vision-Language Models for Robotic Manipulation [[Paper]](https://arxiv.org/abs/2309.02561) [[Project]](https://iliad.stanford.edu/pg-vlm/)
3. (2024.7) **RoboPack**: Learning Tactile-Informed Dynamics Models for Dense Packing [[Paper]](https://arxiv.org/pdf/2407.01418) [[Project]](https://arxiv.org/pdf/2407.01418)



# MLLM + Robotic Manipulation



1. (2023.12) **ManipLLM**: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation [[Paper]](https://arxiv.org/abs/2312.16217) [[Project]](https://sites.google.com/view/manipllm)
2. (2024.3) **ManipVQA**: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models [[Paper]](https://arxiv.org/abs/2405.17418) 
3. (2024.5) **Self-Corrected** Multimodal Large Language Model for End-to-End Robot Manipulation [[Paper]](https://arxiv.org/abs/2405.17418) 
4. (2024.5) **SAM-E**: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation [[Paper]](https://arxiv.org/abs/2405.19586) [[Project]](https://sam-embodied.github.io/)
5. (2024.6) **RoboMamba**: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation [[Paper]](https://arxiv.org/abs/2406.04339) [[Project]](https://sites.google.com/view/robomamba-web)
6. (2024.6) **LLaRA**: Supercharging Robot Learning Data for Vision-Language Policy [[Paper]](https://arxiv.org/pdf/2406.20095) [[Project]](https://github.com/LostXine/LLaRA)
7. (2024.6) **ManiWAV**: Learning Robot Manipulation from In-the-Wild Audio-Visual Data [[Paper]](https://arxiv.org/abs/2406.19464) [[Project]](https://mani-wav.github.io/)





# Reinforcement Learning

1. (2024.6) MEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention [[Paper]](https://arxiv.org/abs/2406.16258)

