# Multimodal Data Learning

1. (2020.11) Diverse Image Captioning with Context-Object Split Latent Spaces [[Paper]](https://arxiv.org/pdf/2011.00966)
2. (2022.12 & 2023.4) Learning Multimodal Data Augmentation in Feature Space [[Paper]](https://arxiv.org/abs/2212.14453)
3. (2024.4) Propensity Score Alignment of Unpaired Multimodal Data [[Paper]](https://arxiv.org/abs/2404.01595v1)
4. (2024.4) TextSquare: Scaling up Text-Centric Visual Instruction Tuning [[Paper]](https://arxiv.org/pdf/2404.12803)
5. (2023.5) **Pre-Trained Vision-Language Models as Partial Annotators** [[Paper]](https://arxiv.org/pdf/2406.18550)
6. (2024.6) Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning [[Paper]](https://arxiv.org/abs/2406.15334) 
7. (2024.6) **Semi-supervised Concept Bottleneck Models** [[Paper]](https://arxiv.org/pdf/2406.18992)
8. (2024.6) **OpenDlign: Enhancing Open-World 3D Learning with Depth-Aligned Images** [[Paper]](https://arxiv.org/abs/2404.16538)
9. (2024.6) From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis [[Paper]](https://arxiv.org/abs/2406.19934) [[Project]](https://github.com/steven-ccq/VisualReasoner)
10. (2024.6) MM-Instruct: Generated Visual Instructions for Large Multimodal Model Alignment [[Paper]](https://arxiv.org/abs/2406.19736) [[Project]](https://github.com/jihaonew/MM-Instruct)
11. (2024.7) Robust Multimodal Learning via Representation Decoupling [[Paper]](https://arxiv.org/pdf/2407.04458)
12. (2024.7) Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding of Unimodal Models [[Paper]](https://arxiv.org/abs/2407.12616v1)
13. (2024.7) Learning Trimodal Relation for Audio-Visual Question Answering with Missing Modality [[Paper]](https://arxiv.org/pdf/2407.16171)





### New learning paradigms



1. (2024.6) Pairwise Difference Learning for Classification [[Paper]](https://arxiv.org/abs/2406.20031v1)