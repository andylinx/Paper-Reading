

## Famous MLLM Base



1. (2023.4) **MiniGPT-4**: Enhancing Vision-Language Understanding with Advanced Large Language Models [[Paper]](https://arxiv.org/abs/2304.10592) [[Project]](https://minigpt-4.github.io/)
2. (2023.5) **LLaVA**: Improved Baselines with Visual Instruction Tuning [[Paper]](https://arxiv.org/abs/2310.03744) [[Project]](https://github.com/haotian-liu/LLaVA)
3. (2023.5) **InstructBLIP**: Towards General-purpose Vision-Language Models with Instruction Tuning [[Paper]](https://arxiv.org/abs/2305.06500) [[Project]](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)
4. (2023.8) **Qwen-VL**: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond [[Paper]](https://arxiv.org/abs/2308.12966) [[Project]](https://github.com/QwenLM/Qwen-VL)
5. (2023.9) **InternLM-XComposer**: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition [[Paper]](https://arxiv.org/abs/2309.15112) [[Project]](https://github.com/InternLM/InternLM-XComposer)
6. (2023.11) **mPLUG-Owl2**: Revolutionizing Multi-modal Large Language Model with Modality Collaboration [[Paper]](https://arxiv.org/abs/2311.04257) [[Project]](https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2)
7. (2024.2) **EVA-CLIP-18B**: Scaling CLIP to 18 Billion Parameters [[Paper]](https://arxiv.org/abs/2402.04252) [[Project]](https://github.com/baaivision/EVA/tree/master/EVA-CLIP-18B)
8. (2024.3) **Cobra**: Extending **Mamba** to Multi-Modal Large Language Model for Efficient Inference [[Paper]](https://arxiv.org/pdf/2403.14520) [[Project]](https://sites.google.com/view/cobravlm)
9. (2024.7) **LLaVA-NeXT-Interleave**: Tackling Multi-image, Video, and 3D in Large Multimodal Models [[Paper]](https://arxiv.org/abs/2407.07895) [[Project]
10. (2024.7) **QWEN2** TECHNICAL REPORT [[Paper]](https://arxiv.org/pdf/2407.10671) [[Blog]](https://qwenlm.github.io/zh/blog/qwen2/)
11. (2024.8) **mPLUG-Owl3**: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models [[Paper]](https://arxiv.org/abs/2408.04840) [[Project]](https://github.com/X-PLUG/mPLUG-Owl)
12. (2024.8) **CogVLM2**: Visual Language Models for Image and Video Understanding [[Paper]](https://arxiv.org/pdf/2408.16500) [[Project]](https://github.com/THUDM/CogVLM2)



## 3D MLLMs

1. (2023.8 & 2023.13) **PointLLM**: Empowering Large Language Models to Understand Point Clouds [[Paper]](https://arxiv.org/abs/2308.16911) [[Project]](https://github.com/OpenRobotLab/PointLLM)
2. (2023.11 & 2024.5) An Embodied Generalist Agent in 3D World [[Paper]](https://arxiv.org/abs/2311.12871) [[Project]](https://embodied-generalist.github.io/)
3. (2023.12) **GPT4Point**: A Unified Framework for Point-Language Understanding and Generation [[Paper]](https://arxiv.org/abs/2312.02980) [[Project]](https://gpt4point.github.io)
4. 



## MLLM Tuning Methods

1. (2022.8) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [[Paper]](https://arxiv.org/abs/2201.11903)
2. (2024.2) Multimodal Instruction Tuning with Conditional Mixture of LoRA [[Paper]](https://arxiv.org/abs/2402.15896)
3. (2024.4) Towards Multimodal In-Context Learning for Vision & Language Models [[Paper]](https://arxiv.org/abs/2403.12736)
4. (2024.4) List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs [[Paper]](https://arxiv.org/abs/2404.16375)
5. (2024.6) MM-Instruct: Generated Visual Instructions for Large Multimodal Model Alignment [[Paper]](https://arxiv.org/abs/2406.19736) [[Project]](https://github.com/jihaonew/MM-Instruct)
6. (2024.7) Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge [[Paper]](https://arxiv.org/pdf/2407.04681)
7. (2024.8) SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs [[Paper]](https://arxiv.org/pdf/2408.11813)





### In-Context Learning

1. (2024.6) Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning [[Paper]](https://arxiv.org/abs/2406.15334) 
1. (2024.8) Making Large Vision Language Models to be Good Few-shot Learners [[Paper]](https://arxiv.org/pdf/2408.11297)



### Omni Models

1. (2024.8) **VITA**: Towards Open-Source Interactive Omni Multimodal LLM [[Paper]](https://arxiv.org/pdf/2408.05211) [[Project]](https://vita-home.github.io/)

