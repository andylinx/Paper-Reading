### Survey

1. (2024.9) From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding [[Paper]](https://arxiv.org/pdf/2409.18938)

​	

## Famous MLLM Base



1. (2023.4) **MiniGPT-4**: Enhancing Vision-Language Understanding with Advanced Large Language Models [[Paper]](https://arxiv.org/abs/2304.10592) [[Project]](https://minigpt-4.github.io/)
2. (2023.5) **LLaVA**: Improved Baselines with Visual Instruction Tuning [[Paper]](https://arxiv.org/abs/2310.03744) [[Project]](https://github.com/haotian-liu/LLaVA)
3. (2023.5) **InstructBLIP**: Towards General-purpose Vision-Language Models with Instruction Tuning [[Paper]](https://arxiv.org/abs/2305.06500) [[Project]](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)
4. (2023.8) **Qwen-VL**: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond [[Paper]](https://arxiv.org/abs/2308.12966) [[Project]](https://github.com/QwenLM/Qwen-VL)
5. (2023.9) **InternLM-XComposer**: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition [[Paper]](https://arxiv.org/abs/2309.15112) [[Project]](https://github.com/InternLM/InternLM-XComposer)
6. (2023.11) **mPLUG-Owl2**: Revolutionizing Multi-modal Large Language Model with Modality Collaboration [[Paper]](https://arxiv.org/abs/2311.04257) [[Project]](https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2)
7. (2023.11) **Video-LLaVA**: Learning United Visual Representation by Alignment Before Projection [[Paper]](https://arxiv.org/abs/2311.10122) 
8. (2024.2) **EVA-CLIP-18B**: Scaling CLIP to 18 Billion Parameters [[Paper]](https://arxiv.org/abs/2402.04252) [[Project]](https://github.com/baaivision/EVA/tree/master/EVA-CLIP-18B)
9. (2024.3) **Cobra**: Extending **Mamba** to Multi-Modal Large Language Model for Efficient Inference [[Paper]](https://arxiv.org/pdf/2403.14520) [[Project]](https://sites.google.com/view/cobravlm)
10. (2024.7) **LLaVA-NeXT-Interleave**: Tackling Multi-image, Video, and 3D in Large Multimodal Models [[Paper]](https://arxiv.org/abs/2407.07895) [[Project]
11. (2024.7) **QWEN2** TECHNICAL REPORT [[Paper]](https://arxiv.org/pdf/2407.10671) [[Blog]](https://qwenlm.github.io/zh/blog/qwen2/)
12. (2024.8) **mPLUG-Owl3**: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models [[Paper]](https://arxiv.org/abs/2408.04840) [[Project]](https://github.com/X-PLUG/mPLUG-Owl)
13. (2024.8) **CogVLM2**: Visual Language Models for Image and Video Understanding [[Paper]](https://arxiv.org/pdf/2408.16500) [[Project]](https://github.com/THUDM/CogVLM2)
14. (2024.9) **NVLM**: Open Frontier-Class Multimodal LLMs [[Paper]](https://arxiv.org/pdf/2409.11402) [[Project]](https://nvlm-project.github.io/)
15. (2024.9) **Qwen2-VL**: Enhancing Vision-Language Model’s Perception of the World at Any Resolution [[Paper]](https://arxiv.org/pdf/2409.12191) [[Project]](https://github.com/QwenLM/Qwen2-VL)
16. (2024.10) **Pixtral 12B** [[Paper]](https://arxiv.org/pdf/2410.07073) [[Project]](https://mistral.ai/news/pixtral-12b/)
17. (2024.10) **Baichuan-Omni** Technical Report [[Paper]](https://arxiv.org/pdf/2410.08565)
18. (2024.12) **NVILA**: Efficient Frontier Visual Language Models [[Paper]](https://arxiv.org/pdf/2412.04468)



## 3D MLLMs

1. (2023.8 & 2023.13) **PointLLM**: Empowering Large Language Models to Understand Point Clouds [[Paper]](https://arxiv.org/abs/2308.16911) [[Project]](https://github.com/OpenRobotLab/PointLLM)
2. (2023.11 & 2024.5) An Embodied Generalist Agent in 3D World [[Paper]](https://arxiv.org/abs/2311.12871) [[Project]](https://embodied-generalist.github.io/)
3. (2023.12) **GPT4Point**: A Unified Framework for Point-Language Understanding and Generation [[Paper]](https://arxiv.org/abs/2312.02980) [[Project]](https://gpt4point.github.io)
4. (2024.9) **LLaVA-3D**: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness [[Paper]](https://arxiv.org/pdf/2409.18125) [[Project]](https://zcmax.github.io/projects/LLaVA-3D/)



## MLLM Tuning Methods

1. (2022.8) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [[Paper]](https://arxiv.org/abs/2201.11903)
2. (2024.2) Multimodal Instruction Tuning with Conditional Mixture of LoRA [[Paper]](https://arxiv.org/abs/2402.15896)
3. (2024.4) Towards Multimodal In-Context Learning for Vision & Language Models [[Paper]](https://arxiv.org/abs/2403.12736)
4. (2024.4) List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs [[Paper]](https://arxiv.org/abs/2404.16375)
5. (2024.6) MM-Instruct: Generated Visual Instructions for Large Multimodal Model Alignment [[Paper]](https://arxiv.org/abs/2406.19736) [[Project]](https://github.com/jihaonew/MM-Instruct)
6. (2024.7) Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge [[Paper]](https://arxiv.org/pdf/2407.04681)
7. (2024.8) SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs [[Paper]](https://arxiv.org/pdf/2408.11813)





### In-Context Learning

1. (2024.6) Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning [[Paper]](https://arxiv.org/abs/2406.15334) 
1. (2024.8) Making Large Vision Language Models to be Good Few-shot Learners [[Paper]](https://arxiv.org/pdf/2408.11297)



#### O1-like Reasoning

1. (2024.11) LLaVA-o1: Let Vision Language Models Reason Step-by-Step [[Paper]](https://arxiv.org/pdf/2411.10440)

### Omni Models

1. (2023.10) **LanguageBind**: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment [[Paper]](https://arxiv.org/abs/2310.01852)
1. (2024.8) **VITA**: Towards Open-Source Interactive Omni Multimodal LLM [[Paper]](https://arxiv.org/pdf/2408.05211) [[Project]](https://vita-home.github.io/)
1. (2024.9) **EMOVA**: Empowering Language Models to See, Hear and Speak with Vivid Emotions [[Paper]](https://arxiv.org/pdf/2409.18042) [[Project]](https://emova-ollm.github.io/)

