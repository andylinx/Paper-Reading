

## Famous MLLM Base



1. (2023.4) **MiniGPT-4**: Enhancing Vision-Language Understanding with Advanced Large Language Models [[Paper]](https://arxiv.org/abs/2304.10592) [[Project]](https://minigpt-4.github.io/)
2. (2023.5) **LLaVA**: Improved Baselines with Visual Instruction Tuning [[Paper]](https://arxiv.org/abs/2310.03744) [[Project]](https://github.com/haotian-liu/LLaVA)
3. (2023.5) **InstructBLIP**: Towards General-purpose Vision-Language Models with Instruction Tuning [[Paper]](https://arxiv.org/abs/2305.06500) [[Project]](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)
4. (2023.8) **Qwen-VL**: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond [[Paper]](https://arxiv.org/abs/2308.12966) [[Project]](https://github.com/QwenLM/Qwen-VL)
5. (2023.9) **InternLM-XComposer**: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition [[Paper]](https://arxiv.org/abs/2309.15112) [[Project]](https://github.com/InternLM/InternLM-XComposer)
6. (2023.11) **mPLUG-Owl2**: Revolutionizing Multi-modal Large Language Model with Modality Collaboration [[Paper]](https://arxiv.org/abs/2311.04257) [[Project]](https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2)
7. (2024.3) **Cobra**: Extending **Mamba** to Multi-Modal Large Language Model for Efficient Inference [[Paper]](https://arxiv.org/pdf/2403.14520) [[Project]](https://sites.google.com/view/cobravlm)
8. (2024.7) **LLaVA-NeXT-Interleave**: Tackling Multi-image, Video, and 3D in Large Multimodal Models [[Paper]](https://arxiv.org/abs/2407.07895) [[Project]
9. (2024.7) **QWEN2** TECHNICAL REPORT [[Paper]](https://arxiv.org/pdf/2407.10671) [[Blog]](https://qwenlm.github.io/zh/blog/qwen2/)

## MLLM Tuning Methods

1. (2022.8) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [[Paper]](https://arxiv.org/abs/2201.11903)
2. (2024.2) Multimodal Instruction Tuning with Conditional Mixture of LoRA [[Paper]](https://arxiv.org/abs/2402.15896)
3. (2024.4) Towards Multimodal In-Context Learning for Vision & Language Models [[Paper]](https://arxiv.org/abs/2403.12736)
4. (2024.4) List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs [[Paper]](https://arxiv.org/abs/2404.16375)
5. (2024.7) Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge [[Paper]](https://arxiv.org/pdf/2407.04681)

